# Meant to be used to analyze the PBMC dataset
additional_worker_groups: []
jupyter:
  affinity: {}
  args: null
  command: null
  enabled: true
  env:
    #- name: EXTRA_PIP_PACKAGES
    #  value: pyscenic
    - name: DATABASES_PATH
      value: &jup_path "/mnt/databases"
  extraConfig: |-
    # Extra Jupyter config goes here
    # E.g
    # c.NotebookApp.port = 8888
  image:
    pullPolicy: IfNotPresent
    pullSecrets: null
    repository: carloskez/dask_jupyter@sha256
    tag: 8a379f45cc4ae87cb272f1ac7c8045e311680b9fed8ce0312e405ada3d82d531
  ingress:
    annotations: 
      nginx.ingress.kubernetes.io/proxy-body-size: 500m
    enabled: true
    ingressClassName: nginx
    hostname: jupyter-dsk.mooo.com
    pathType: Prefix
    tls: true
    secretName: dask-tls
  livenessProbe: {}
  mounts: # Worker Pod volumes and volume mounts, mounts.volumes follows kuberentes api v1 Volumes spec. mounts.volumeMounts follows kubernetesapi v1 VolumeMount spec
    volumes:
      - name: dask-storage
        persistentVolumeClaim:
          claimName: dask-pv-claim
      - name: dask-nfs-storage
        persistentVolumeClaim:
          claimName: dask-nfs-claim
    volumeMounts:
      - name: dask-storage
        mountPath: /home/jovyan/work # folder for storage
      - name: dask-nfs-storage
        mountPath: *jup_path # database folder
  name: jupyter
  nodeSelector: {}
  password: argon2:$argon2id$v=19$m=10240,t=10,p=8$UlwKTq+3lrh+Z3KSkPzr2g$BnZu0jtB8V+mbwD4Zt6GdzyTzhG9dXxvQDanCa3lXro
  rbac: true
  readinessProbe: {}
  resources: 
    limits:
        cpu: 2
        memory: 10G
    requests:
        cpu: 1
        memory: 4G
  securityContext: {}
  serviceAccountName: dask-jupyter
  servicePort: 80
  serviceType: ClusterIP
  tolerations: []
scheduler:
  affinity: {}
  enabled: true
  extraArgs: []
  image:
    pullPolicy: IfNotPresent
    pullSecrets: null
    repository: carloskez/dask_pyscenic@sha256
    tag: a55155b49c3ab66a40dc5f6d07fd2e1c417275560de62c87ba0c4394ac9a7c31 #debug image
  livenessProbe: {}
  loadBalancerIP: null
  metrics:
    enabled: false
    serviceMonitor:
      additionalLabels: {}
      enabled: false
      interval: 30s
      jobLabel: ""
      metricRelabelings: []
      namespace: ""
      namespaceSelector: {}
      targetLabels: []
  name: scheduler
  nodeSelector: {}
  readinessProbe: {}
  replicas: 1
  resources: {}
  securityContext: {}
  serviceAnnotations: {}
  servicePort: 8786
  serviceType: ClusterIP
  tolerations: []
webUI:
  ingress:
    annotations: {}
    enabled: true
    ingressClassName: nginx
    hostname: dask-dsk.mooo.com
    pathType: Prefix
    tls: true
    secretName: dask-tls
  name: webui
  servicePort: 80
worker:
  affinity: {}
  annotations: {}
  custom_scheduler_url: null
  default_resources:
    cpu: 1
    memory: 4GiB
  env: 
    - name: DATABASES_PATH
      value: &worker_path "/mnt/databases"
  extraArgs: []
  image:
    dask_worker: dask-worker
    pullPolicy: IfNotPresent
    pullSecrets: null
    repository: carloskez/dask_pyscenic@sha256
    tag: a55155b49c3ab66a40dc5f6d07fd2e1c417275560de62c87ba0c4394ac9a7c31 #debug image
  livenessProbe: {}
  metrics:
    enabled: false
    podMonitor:
      additionalLabels: {}
      enabled: false
      interval: 30s
      jobLabel: ""
      metricRelabelings: []
      namespace: ""
      namespaceSelector: {}
      podTargetLabels: []
  mounts: 
    volumes:
      - name: dask-nfs-storage
        persistentVolumeClaim:
          claimName: dask-nfs-claim
    volumeMounts:      
      - name: dask-nfs-storage
        mountPath: *worker_path # database folder    
  name: worker
  nodeSelector: {}
  portDashboard: 8790
  readinessProbe: {}
  replicas: 8
  resources: 
    limits:
      cpu: 1
      memory: 6G
    requests:
      cpu: 1
      memory: 6G
  securityContext: {}
  strategy:
    type: RollingUpdate
  tolerations: []